{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fomo.models.clip.clip_base import ClipBase\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Base Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip = ClipBase()\n",
    "clip.to_cpu()\n",
    "clip.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.precompute_prompt_features([\"a picture of a cat\", \"a picture of kitten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = clip.transform(img).view(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    logits = clip.forward(img_tensor)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.functional.F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fomo.utils.data.datasets import DatasetInitializer\n",
    "from fomo.utils.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading zero_shot_dataset\n",
    "zero_shot_dataset = DatasetInitializer.from_str(\"cifar10\").value(train=True)\n",
    "\n",
    "# zero_shot_dataset contains two properties: torch dataset and labels\n",
    "print(zero_shot_dataset.dataset)\n",
    "print(zero_shot_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can also split the dataset into train and eval splits using utils\n",
    "\n",
    "# using percentage split\n",
    "train_dataset, val_dataset = utils.split_train_val(zero_shot_dataset.dataset, train_size=0.8)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using number of samples split\n",
    "\n",
    "train_dataset, val_dataset = utils.split_train_val(zero_shot_dataset.dataset, train_eval_samples=[10, 20])\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "25000\n",
      "25000\n"
     ]
    }
   ],
   "source": [
    "# using subsample split\n",
    "subsampled_dataset = utils.subsample_classes(zero_shot_dataset.dataset, \"all\")\n",
    "print(subsampled_dataset.__len__())\n",
    "\n",
    "subsampled_dataset = utils.subsample_classes(zero_shot_dataset.dataset, \"base\")\n",
    "print(subsampled_dataset.__len__())\n",
    "\n",
    "subsampled_dataset = utils.subsample_classes(zero_shot_dataset.dataset, \"new\")\n",
    "print(subsampled_dataset.__len__())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fomo.pipelines.train import Learner\n",
    "from fomo.pipelines.types.learner_args import LearnerArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner_args = LearnerArgs()\n",
    "learner_args.device = \"cpu\"\n",
    "learner_args.epochs = 1\n",
    "learner_args.model_type = \"clip_linear\"\n",
    "learner_args.use_wandb = True\n",
    "learner_args.train_subsample = \"base\"\n",
    "learner_args.test_subsample = \"new\"\n",
    "learner_args.train_eval_size = (10, 10)\n",
    "\n",
    "learner = Learner(learner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending ClipBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ClipBase\n",
    "from fomo.models.clip.clip_base import ClipBase\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipExtension(ClipBase):\n",
    "    def __init__(self, backbone: str = \"ViT-B/16\", root: str = \"./data\") -> None:\n",
    "        # pass default arguments to the parent class\n",
    "        super(ClipExtension, self).__init__(backbone, root=root)\n",
    "\n",
    "        # add additional blocks to the model\n",
    "\n",
    "        self.visual_mlp = nn.Sequential(\n",
    "            nn.Linear(self._clip.visual.output_dim, 12),\n",
    "            nn.Linear(12, self._clip.visual.output_dim)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def learnable_param_names(self) -> set[str]:\n",
    "         # IMPORTANT: Add the name of the learnable parameters in the model\n",
    "        return set([\"image_linear\"])\n",
    "\n",
    "    # If needed you can override the to_cpu and to_cuda methods\n",
    "    def to_cpu(self) -> None:\n",
    "        self._clip.to(torch.device(\"cpu\"))\n",
    "        self.image_linear.to(torch.device(\"cpu\"))\n",
    "        self._clip.float()\n",
    "\n",
    "    def to_cuda(self) -> None:\n",
    "        self.image_linear.to(torch.device(\"cuda\"))\n",
    "        self._clip.to(torch.device(\"cuda\"))\n",
    "\n",
    "    def forward(self, images: torch.Tensor, prompts: list[str] | None = None) -> torch.Tensor:\n",
    "        # Change the forward method to include the visual_mlp\n",
    "        if prompts:\n",
    "            text_features = self.encode_text(prompts)\n",
    "        elif self._precomputed_prompt_features is not None:\n",
    "            text_features = self._precomputed_prompt_features\n",
    "        else:\n",
    "            raise ValueError(\"At least one prompts or pre-computed promt features has to be present.\")\n",
    "\n",
    "        image_features = self.encode_images(images)\n",
    "\n",
    "        image_features = self.image_linear(image_features)\n",
    "\n",
    "        logits_per_image: torch.Tensor = self.logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClipExtension()\n",
    "\n",
    "#print learnable parameters\n",
    "print(model.learnable_param_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### N-class K-shot dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from fomo.pipelines.train import Learner\n",
    "from fomo.pipelines.types.learner_args import LearnerArgs\n",
    "from fomo.pipelines.utils.initializers import initalize_datasets, initalize_n_class_k_shot_dataloaders, intialize_model\n",
    "\n",
    "learner_args = LearnerArgs()\n",
    "learner_args.device = \"mps\"\n",
    "# learner_args.epochs = 300\n",
    "# learner_args.patience = 10\n",
    "# learner_args.print_freq = 20\n",
    "# learner_args.save_freq = 300\n",
    "\n",
    "learner_args.model_type = \"clip_transformer\"\n",
    "learner_args.dataloder_type = 'n_class_k_shot'\n",
    "learner_args.n = 5\n",
    "learner_args.k = 16\n",
    "# learner_args.train_size = 0.8\n",
    "learner_args.train_eval_size = [100,20]\n",
    "\n",
    "dataset = DatasetInitializer.from_str(\"cifar10\").value(train=True)\n",
    "\n",
    "# Load clip image transformation\n",
    "model = intialize_model(\n",
    "    learner_args.model_type, learner_args.model_backbone, learner_args.device\n",
    ")\n",
    "transforms = model.transforms\n",
    "\n",
    "(train_dataset, test_dataset), labels = initalize_datasets(learner_args.dataset, transforms)\n",
    "\n",
    "train_loader, val_loader, test_loader = initalize_n_class_k_shot_dataloaders(\n",
    "    train_dataset, test_dataset, learner_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'mm_to_text_mlp.0.weight', 'mm_to_visual_mlp.2.weight', 'mm_to_text_mlp.2.weight', 'mm_to_visual_mlp.0.weight'}\n",
      "Number of learnable paramms: 98304\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:x7yw048o) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fomo.pipelines.train import Learner\n",
    "from fomo.pipelines.types.learner_args import LearnerArgs\n",
    "\n",
    "learner_args = LearnerArgs()\n",
    "learner_args.epochs = 100\n",
    "learner_args.patience = 20\n",
    "learner_args.print_freq = 50\n",
    "learner_args.save_freq = 200\n",
    "learner_args.learning_rate = 0.02\n",
    "learner_args.momentum = 0.9\n",
    "learner_args.weight_decay =  0.002\n",
    "learner_args.use_wandb = True\n",
    "learner_args.dataset = 'oxford_pets'\n",
    "\n",
    "learner_args.model_type = \"clip_mm_mlp_adapter\"\n",
    "# learner_args.dataloder_type = 'n_class_k_shot'\n",
    "# learner_args.n = 5\n",
    "# learner_args.k = 16\n",
    "learner_args.train_size = None # 0.8\n",
    "learner_args.train_eval_size = [592,592]\n",
    "learner_args.batch_size = 64\n",
    "\n",
    "\n",
    "learner = Learner(learner_args)\n",
    "\n",
    "learner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
