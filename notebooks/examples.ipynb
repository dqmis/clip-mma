{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fomo.models.clip.clip_base import ClipBase\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLIP Base Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/clip/clip.py:57: UserWarning: ./data/ViT-B-16.pt exists, but the SHA256 checksum does not match; re-downloading the file\n",
      "  warnings.warn(f\"{download_target} exists, but the SHA256 checksum does not match; re-downloading the file\")\n",
      "100%|███████████████████████████████████████| 335M/335M [02:33<00:00, 2.29MiB/s]\n"
     ]
    }
   ],
   "source": [
    "clip = ClipBase()\n",
    "clip.to_cpu()\n",
    "clip.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "clip.precompute_prompt_features([\"a picture of a cat\", \"a picture of kitten\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = Image.open(\"./cat.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_tensor = clip.transform(img).view(1, 3, 224, 224)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[29.0192, 26.0525]])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    logits = clip.forward(img_tensor)\n",
    "print(logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9510, 0.0490]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.functional.F.softmax(logits, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fomo.utils.data.datasets import DatasetInitializer\n",
    "from fomo.utils.data import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: data\n",
      "    Split: Train\n",
      "['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n"
     ]
    }
   ],
   "source": [
    "# loading zero_shot_dataset\n",
    "zero_shot_dataset = DatasetInitializer.from_str(\"cifar10\").value(train=True)\n",
    "\n",
    "# zero_shot_dataset contains two properties: torch dataset and labels\n",
    "print(zero_shot_dataset.dataset)\n",
    "print(zero_shot_dataset.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40000 10000\n"
     ]
    }
   ],
   "source": [
    "# you can also split the dataset into train and eval splits using utils\n",
    "\n",
    "# using percentage split\n",
    "train_dataset, val_dataset = utils.split_train_val(zero_shot_dataset.dataset, train_size=0.8)\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 20\n"
     ]
    }
   ],
   "source": [
    "# using number of samples split\n",
    "train_dataset, val_dataset = utils.split_train_val(zero_shot_dataset.dataset, train_eval_samples=[10, 20])\n",
    "print(len(train_dataset), len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from fomo.pipelines.train import Learner\n",
    "from fomo.pipelines.types.learner_args import LearnerArgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Turning off gradients in both the image and the text encoder\n",
      "Parameters to be updated: {'image_linear.0.bias', 'image_linear.0.weight'}\n",
      "Number of learnable paramms: 262656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learner_args = LearnerArgs()\n",
    "learner_args.device = \"cpu\"\n",
    "learner_args.epochs = 2\n",
    "learner_args.model_type = \"clip_linear\"\n",
    "learner_args.train_eval_size = (10, 10)\n",
    "\n",
    "learner = Learner(learner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/1]\tTime  8.351 ( 8.351)\tData  7.830 ( 7.830)\tLoss 3.1633e+00 (3.1633e+00)\tAcc@1   0.00 (  0.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:14<00:00, 14.31s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/1]\tTime  8.214 ( 8.214)\tLoss 2.3106e+00 (2.3106e+00)\tPrompt Acc@1  10.00 ( 10.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 10.000\n",
      "saved best file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/1]\tTime  8.372 ( 8.372)\tData  7.829 ( 7.829)\tLoss 2.3008e+00 (2.3008e+00)\tAcc@1  10.00 ( 10.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.95s/it]\n",
      "100%|██████████| 1/1 [00:08<00:00,  8.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/1]\tTime  8.202 ( 8.202)\tLoss 2.3206e+00 (2.3206e+00)\tPrompt Acc@1  10.00 ( 10.00)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:13<00:00, 13.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 10.000\n",
      "There's no improvement for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/157 [00:08<22:58,  8.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [  0/157]\tTime  8.839 ( 8.839)\tLoss 2.3017e+00 (2.3017e+00)\tPrompt Acc@1  14.06 ( 14.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/157 [00:38<07:29,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 10/157]\tTime  3.080 ( 3.537)\tLoss 2.3043e+00 (2.2982e+00)\tPrompt Acc@1   7.81 ( 10.65)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 21/157 [01:09<06:58,  3.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 20/157]\tTime  2.972 ( 3.311)\tLoss 2.2972e+00 (2.2991e+00)\tPrompt Acc@1  12.50 (  9.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 31/157 [01:40<06:25,  3.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 30/157]\tTime  2.970 ( 3.241)\tLoss 2.3166e+00 (2.3019e+00)\tPrompt Acc@1   3.12 (  9.07)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 41/157 [02:09<05:40,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 40/157]\tTime  2.909 ( 3.170)\tLoss 2.3203e+00 (2.3009e+00)\tPrompt Acc@1   4.69 (  9.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 51/157 [02:39<05:13,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 50/157]\tTime  2.960 ( 3.128)\tLoss 2.3281e+00 (2.3030e+00)\tPrompt Acc@1   9.38 (  9.34)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 61/157 [03:09<04:44,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 60/157]\tTime  2.931 ( 3.101)\tLoss 2.2982e+00 (2.3038e+00)\tPrompt Acc@1  14.06 (  9.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 71/157 [03:38<04:14,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 70/157]\tTime  3.012 ( 3.082)\tLoss 2.3027e+00 (2.3037e+00)\tPrompt Acc@1   9.38 (  9.40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 81/157 [04:08<03:44,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 80/157]\tTime  2.932 ( 3.067)\tLoss 2.2922e+00 (2.3044e+00)\tPrompt Acc@1  10.94 (  9.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 91/157 [04:37<03:14,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 90/157]\tTime  2.958 ( 3.053)\tLoss 2.3166e+00 (2.3061e+00)\tPrompt Acc@1   6.25 (  9.13)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 101/157 [05:07<02:45,  2.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [100/157]\tTime  2.947 ( 3.043)\tLoss 2.2895e+00 (2.3067e+00)\tPrompt Acc@1   7.81 (  9.24)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 111/157 [05:36<02:16,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [110/157]\tTime  2.988 ( 3.035)\tLoss 2.3122e+00 (2.3063e+00)\tPrompt Acc@1   9.38 (  9.22)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 121/157 [06:06<01:45,  2.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [120/157]\tTime  2.895 ( 3.027)\tLoss 2.2964e+00 (2.3063e+00)\tPrompt Acc@1   9.38 (  9.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 131/157 [06:35<01:16,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [130/157]\tTime  2.976 ( 3.021)\tLoss 2.3266e+00 (2.3060e+00)\tPrompt Acc@1   6.25 (  9.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 141/157 [07:05<00:47,  2.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [140/157]\tTime  2.973 ( 3.017)\tLoss 2.2835e+00 (2.3052e+00)\tPrompt Acc@1   9.38 (  9.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 151/157 [07:34<00:17,  2.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [150/157]\tTime  2.913 ( 3.013)\tLoss 2.2895e+00 (2.3050e+00)\tPrompt Acc@1   6.25 (  9.41)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [08:10<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 9.430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learner.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extending ClipBase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ClipBase\n",
    "from fomo.models.clip.clip_base import ClipBase\n",
    "from torch import nn\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipExtension(ClipBase):\n",
    "    def __init__(self, backbone: str = \"ViT-B/16\", root: str = \"./data\") -> None:\n",
    "        # pass default arguments to the parent class\n",
    "        super(ClipExtension, self).__init__(backbone, root=root)\n",
    "\n",
    "        # add additional blocks to the model\n",
    "\n",
    "        self.visual_mlp = nn.Sequential(\n",
    "            nn.Linear(self._clip.visual.output_dim, 12),\n",
    "            nn.Linear(12, self._clip.visual.output_dim)\n",
    "        )\n",
    "\n",
    "    @property\n",
    "    def learnable_param_names(self) -> set[str]:\n",
    "         # IMPORTANT: Add the name of the learnable parameters in the model\n",
    "        return set([\"image_linear\"])\n",
    "\n",
    "    # If needed you can override the to_cpu and to_cuda methods\n",
    "    def to_cpu(self) -> None:\n",
    "        self._clip.to(torch.device(\"cpu\"))\n",
    "        self.image_linear.to(torch.device(\"cpu\"))\n",
    "        self._clip.float()\n",
    "\n",
    "    def to_cuda(self) -> None:\n",
    "        self.image_linear.to(torch.device(\"cuda\"))\n",
    "        self._clip.to(torch.device(\"cuda\"))\n",
    "\n",
    "    def forward(self, images: torch.Tensor, prompts: list[str] | None = None) -> torch.Tensor:\n",
    "        # Change the forward method to include the visual_mlp\n",
    "        if prompts:\n",
    "            text_features = self.encode_text(prompts)\n",
    "        elif self._precomputed_prompt_features is not None:\n",
    "            text_features = self._precomputed_prompt_features\n",
    "        else:\n",
    "            raise ValueError(\"At least one prompts or pre-computed promt features has to be present.\")\n",
    "\n",
    "        image_features = self.encode_images(images)\n",
    "\n",
    "        image_features = self.image_linear(image_features)\n",
    "\n",
    "        logits_per_image: torch.Tensor = self.logit_scale * image_features @ text_features.t()\n",
    "\n",
    "        return logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_linear'}\n"
     ]
    }
   ],
   "source": [
    "model = ClipExtension()\n",
    "\n",
    "#print learnable parameters\n",
    "print(model.learnable_param_names)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
