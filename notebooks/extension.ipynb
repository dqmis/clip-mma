{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ClipBase\n",
    "from fomo.models.clip.clip_base import ClipBase\n",
    "from torch import nn\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClipBase()\n",
    "model.to_cpu()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 2, 512])\n"
     ]
    }
   ],
   "source": [
    "img1 = Image.open(\"./cat.jpg\")\n",
    "img2 = Image.open(\"./dog.jpg\")\n",
    "img1_tensor = model.transform(img1)\n",
    "img2_tensor = model.transform(img2)\n",
    "\n",
    "img_input = torch.stack([img1_tensor, img2_tensor])\n",
    "\n",
    "img_emb = model.encode_images(img_input).unsqueeze(0)\n",
    "print(img_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\"]\n",
    "prompt_emb = model.encode_text(prompts)\n",
    "prompt_emb = prompt_emb.unsqueeze(1).expand(-1, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 2, 512])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input = torch.cat([img_emb, prompt_emb], dim=0)\n",
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MiniTransformer(nn.Module):\n",
    "    def __init__(self, num_classes: int = 5) -> None:\n",
    "        super(MiniTransformer, self).__init__()\n",
    "\n",
    "        self.wq = nn.Sequential(\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Linear(2, 512),\n",
    "        )\n",
    "        self.wk = nn.Sequential(\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Linear(2, 512),\n",
    "        )\n",
    "        self.wv = nn.Sequential(\n",
    "            nn.Linear(512, 2),\n",
    "            nn.Linear(2, 512),\n",
    "        )\n",
    "        self.transformer = nn.MultiheadAttention(embed_dim=512, num_heads=2)\n",
    "        self._attn_mask = self._init_attn_mask(num_classes, 1)\n",
    "\n",
    "    @staticmethod\n",
    "    def _init_attn_mask(num_prompts: int, num_images: int) -> torch.Tensor:\n",
    "        num_total = num_prompts + num_images\n",
    "        mask = torch.zeros((num_total, num_total))\n",
    "\n",
    "        for i in range(num_prompts):\n",
    "            for j in range(num_prompts, num_total):\n",
    "                mask[i, j] = 1\n",
    "\n",
    "        for i in range(num_prompts, num_total):\n",
    "            for j in range(num_prompts):\n",
    "                mask[i, j] = 1\n",
    "\n",
    "        return mask\n",
    "\n",
    "    def forward(self, inputs: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size = inputs.shape[1]\n",
    "        query = self.wq(inputs)\n",
    "        key = self.wk(inputs)\n",
    "        value = self.wv(inputs)\n",
    "\n",
    "        mask = self._attn_mask.clone().detach().unsqueeze(0).repeat(batch_size*2, 1, 1)\n",
    "\n",
    "        return self.transformer(query, key, value, attn_mask=mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.functional import F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClipExtension(ClipBase):\n",
    "    def __init__(self, backbone: str = \"ViT-B/16\", root: str = \"./data\", num_classes: int = 10) -> None:\n",
    "        # pass default arguments to the parent class\n",
    "        super(ClipExtension, self).__init__(backbone, root=root)\n",
    "\n",
    "        self._num_classes = num_classes\n",
    "        self.transformer = MiniTransformer(num_classes=self._num_classes)\n",
    "\n",
    "    @property\n",
    "    def learnable_param_names(self) -> set[str]:\n",
    "         # IMPORTANT: Add the name of the learnable parameters in the model\n",
    "        return set([\"transformer\"])\n",
    "\n",
    "    # If needed you can override the to_cpu and to_cuda methods\n",
    "    def to_cpu(self) -> None:\n",
    "        self._clip.to(torch.device(\"cpu\"))\n",
    "        self.transformer.to(torch.device(\"cpu\"))\n",
    "        self._clip.float()\n",
    "\n",
    "    def to_cuda(self) -> None:\n",
    "        self.transformer.to(torch.device(\"cuda\"))\n",
    "        self._clip.to(torch.device(\"cuda\"))\n",
    "\n",
    "    def forward(self, images: torch.Tensor, prompts: list[str] | None = None) -> torch.Tensor:\n",
    "        # Change the forward method to include the visual_mlp\n",
    "        if prompts:\n",
    "            text_features = self.encode_text(prompts)\n",
    "        elif self._precomputed_prompt_features is not None:\n",
    "            text_features = self._precomputed_prompt_features\n",
    "        else:\n",
    "            raise ValueError(\"At least one prompts or pre-computed promt features has to be present.\")\n",
    "\n",
    "        image_features = self.encode_images(images).unsqueeze(0)\n",
    "        text_features = text_features.unsqueeze(1).expand(-1, image_features.shape[1], -1)\n",
    "\n",
    "        inputs =  torch.cat([text_features, image_features], dim=0)\n",
    "        tr_outputs = self.transformer(inputs)[0]\n",
    "\n",
    "        inputs += tr_outputs\n",
    "\n",
    "        image_features = inputs[self._num_classes:]\n",
    "        text_features = inputs[:self._num_classes]\n",
    "\n",
    "        images = image_features.permute(1, 0, 2)\n",
    "        prompts = text_features.permute(1, 0, 2)\n",
    "\n",
    "        logits_per_image: torch.Tensor = torch.bmm(prompts, images.transpose(1, 2))\n",
    "\n",
    "        return logits_per_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClipExtension()\n",
    "model.to_cpu()\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.precompute_prompt_features([\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = model.forward(img_input).squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3497, 0.3201, 0.3302],\n",
       "        [0.3273, 0.3431, 0.3296]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.softmax(out, dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mmha.mha.in_proj_weight\n",
      "mmha.mha.in_proj_bias\n",
      "mmha.mha.out_proj.weight\n",
      "mmha.mha.out_proj.bias\n"
     ]
    }
   ],
   "source": [
    "from fomo.models.clip.clip_transformer import ClipTransformer\n",
    "from PIL import Image\n",
    "import torch\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "clip = ClipTransformer()\n",
    "clip.to_cpu()\n",
    "clip.precompute_prompt_features([\"a photo of a cat\", \"a photo of a dog\", \"a photo of a bird\"])\n",
    "\n",
    "for name, param in clip.named_parameters():\n",
    "    param.requires_grad = False\n",
    "    for learnable_param_name in clip.learnable_param_names:\n",
    "        if learnable_param_name in name:\n",
    "            param.requires_grad = True\n",
    "\n",
    "# print the learnable parameters\n",
    "for name, param in clip.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name)\n",
    "\n",
    "\n",
    "\n",
    "img1 = Image.open(\"./cat.jpg\")\n",
    "img2 = Image.open(\"./dog.jpg\")\n",
    "img1_tensor = clip.transform(img1)\n",
    "img2_tensor = clip.transform(img2)\n",
    "\n",
    "img_input = torch.stack([img1_tensor, img2_tensor])\n",
    "\n",
    "out = clip.forward(img_input).squeeze(-1)\n",
    "\n",
    "loss = torch.functional.F.cross_entropy(out, torch.tensor([0, 1]))\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fomo.pipelines.train import Learner\n",
    "from fomo.pipelines.types.learner_args import LearnerArgs\n",
    "import torch\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Turning off gradients in both the image and the text encoder\n",
      "['_clip.positional_embedding', '_clip.text_projection', '_clip.logit_scale', '_clip.visual.class_embedding', '_clip.visual.positional_embedding', '_clip.visual.proj', '_clip.visual.conv1.weight', '_clip.visual.ln_pre.weight', '_clip.visual.ln_pre.bias', '_clip.visual.transformer.resblocks.0.attn.in_proj_weight', '_clip.visual.transformer.resblocks.0.attn.in_proj_bias', '_clip.visual.transformer.resblocks.0.attn.out_proj.weight', '_clip.visual.transformer.resblocks.0.attn.out_proj.bias', '_clip.visual.transformer.resblocks.0.ln_1.weight', '_clip.visual.transformer.resblocks.0.ln_1.bias', '_clip.visual.transformer.resblocks.0.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.0.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.0.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.0.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.0.ln_2.weight', '_clip.visual.transformer.resblocks.0.ln_2.bias', '_clip.visual.transformer.resblocks.1.attn.in_proj_weight', '_clip.visual.transformer.resblocks.1.attn.in_proj_bias', '_clip.visual.transformer.resblocks.1.attn.out_proj.weight', '_clip.visual.transformer.resblocks.1.attn.out_proj.bias', '_clip.visual.transformer.resblocks.1.ln_1.weight', '_clip.visual.transformer.resblocks.1.ln_1.bias', '_clip.visual.transformer.resblocks.1.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.1.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.1.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.1.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.1.ln_2.weight', '_clip.visual.transformer.resblocks.1.ln_2.bias', '_clip.visual.transformer.resblocks.2.attn.in_proj_weight', '_clip.visual.transformer.resblocks.2.attn.in_proj_bias', '_clip.visual.transformer.resblocks.2.attn.out_proj.weight', '_clip.visual.transformer.resblocks.2.attn.out_proj.bias', '_clip.visual.transformer.resblocks.2.ln_1.weight', '_clip.visual.transformer.resblocks.2.ln_1.bias', '_clip.visual.transformer.resblocks.2.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.2.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.2.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.2.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.2.ln_2.weight', '_clip.visual.transformer.resblocks.2.ln_2.bias', '_clip.visual.transformer.resblocks.3.attn.in_proj_weight', '_clip.visual.transformer.resblocks.3.attn.in_proj_bias', '_clip.visual.transformer.resblocks.3.attn.out_proj.weight', '_clip.visual.transformer.resblocks.3.attn.out_proj.bias', '_clip.visual.transformer.resblocks.3.ln_1.weight', '_clip.visual.transformer.resblocks.3.ln_1.bias', '_clip.visual.transformer.resblocks.3.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.3.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.3.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.3.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.3.ln_2.weight', '_clip.visual.transformer.resblocks.3.ln_2.bias', '_clip.visual.transformer.resblocks.4.attn.in_proj_weight', '_clip.visual.transformer.resblocks.4.attn.in_proj_bias', '_clip.visual.transformer.resblocks.4.attn.out_proj.weight', '_clip.visual.transformer.resblocks.4.attn.out_proj.bias', '_clip.visual.transformer.resblocks.4.ln_1.weight', '_clip.visual.transformer.resblocks.4.ln_1.bias', '_clip.visual.transformer.resblocks.4.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.4.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.4.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.4.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.4.ln_2.weight', '_clip.visual.transformer.resblocks.4.ln_2.bias', '_clip.visual.transformer.resblocks.5.attn.in_proj_weight', '_clip.visual.transformer.resblocks.5.attn.in_proj_bias', '_clip.visual.transformer.resblocks.5.attn.out_proj.weight', '_clip.visual.transformer.resblocks.5.attn.out_proj.bias', '_clip.visual.transformer.resblocks.5.ln_1.weight', '_clip.visual.transformer.resblocks.5.ln_1.bias', '_clip.visual.transformer.resblocks.5.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.5.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.5.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.5.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.5.ln_2.weight', '_clip.visual.transformer.resblocks.5.ln_2.bias', '_clip.visual.transformer.resblocks.6.attn.in_proj_weight', '_clip.visual.transformer.resblocks.6.attn.in_proj_bias', '_clip.visual.transformer.resblocks.6.attn.out_proj.weight', '_clip.visual.transformer.resblocks.6.attn.out_proj.bias', '_clip.visual.transformer.resblocks.6.ln_1.weight', '_clip.visual.transformer.resblocks.6.ln_1.bias', '_clip.visual.transformer.resblocks.6.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.6.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.6.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.6.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.6.ln_2.weight', '_clip.visual.transformer.resblocks.6.ln_2.bias', '_clip.visual.transformer.resblocks.7.attn.in_proj_weight', '_clip.visual.transformer.resblocks.7.attn.in_proj_bias', '_clip.visual.transformer.resblocks.7.attn.out_proj.weight', '_clip.visual.transformer.resblocks.7.attn.out_proj.bias', '_clip.visual.transformer.resblocks.7.ln_1.weight', '_clip.visual.transformer.resblocks.7.ln_1.bias', '_clip.visual.transformer.resblocks.7.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.7.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.7.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.7.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.7.ln_2.weight', '_clip.visual.transformer.resblocks.7.ln_2.bias', '_clip.visual.transformer.resblocks.8.attn.in_proj_weight', '_clip.visual.transformer.resblocks.8.attn.in_proj_bias', '_clip.visual.transformer.resblocks.8.attn.out_proj.weight', '_clip.visual.transformer.resblocks.8.attn.out_proj.bias', '_clip.visual.transformer.resblocks.8.ln_1.weight', '_clip.visual.transformer.resblocks.8.ln_1.bias', '_clip.visual.transformer.resblocks.8.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.8.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.8.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.8.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.8.ln_2.weight', '_clip.visual.transformer.resblocks.8.ln_2.bias', '_clip.visual.transformer.resblocks.9.attn.in_proj_weight', '_clip.visual.transformer.resblocks.9.attn.in_proj_bias', '_clip.visual.transformer.resblocks.9.attn.out_proj.weight', '_clip.visual.transformer.resblocks.9.attn.out_proj.bias', '_clip.visual.transformer.resblocks.9.ln_1.weight', '_clip.visual.transformer.resblocks.9.ln_1.bias', '_clip.visual.transformer.resblocks.9.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.9.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.9.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.9.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.9.ln_2.weight', '_clip.visual.transformer.resblocks.9.ln_2.bias', '_clip.visual.transformer.resblocks.10.attn.in_proj_weight', '_clip.visual.transformer.resblocks.10.attn.in_proj_bias', '_clip.visual.transformer.resblocks.10.attn.out_proj.weight', '_clip.visual.transformer.resblocks.10.attn.out_proj.bias', '_clip.visual.transformer.resblocks.10.ln_1.weight', '_clip.visual.transformer.resblocks.10.ln_1.bias', '_clip.visual.transformer.resblocks.10.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.10.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.10.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.10.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.10.ln_2.weight', '_clip.visual.transformer.resblocks.10.ln_2.bias', '_clip.visual.transformer.resblocks.11.attn.in_proj_weight', '_clip.visual.transformer.resblocks.11.attn.in_proj_bias', '_clip.visual.transformer.resblocks.11.attn.out_proj.weight', '_clip.visual.transformer.resblocks.11.attn.out_proj.bias', '_clip.visual.transformer.resblocks.11.ln_1.weight', '_clip.visual.transformer.resblocks.11.ln_1.bias', '_clip.visual.transformer.resblocks.11.mlp.c_fc.weight', '_clip.visual.transformer.resblocks.11.mlp.c_fc.bias', '_clip.visual.transformer.resblocks.11.mlp.c_proj.weight', '_clip.visual.transformer.resblocks.11.mlp.c_proj.bias', '_clip.visual.transformer.resblocks.11.ln_2.weight', '_clip.visual.transformer.resblocks.11.ln_2.bias', '_clip.visual.ln_post.weight', '_clip.visual.ln_post.bias', '_clip.transformer.resblocks.0.attn.in_proj_weight', '_clip.transformer.resblocks.0.attn.in_proj_bias', '_clip.transformer.resblocks.0.attn.out_proj.weight', '_clip.transformer.resblocks.0.attn.out_proj.bias', '_clip.transformer.resblocks.0.ln_1.weight', '_clip.transformer.resblocks.0.ln_1.bias', '_clip.transformer.resblocks.0.mlp.c_fc.weight', '_clip.transformer.resblocks.0.mlp.c_fc.bias', '_clip.transformer.resblocks.0.mlp.c_proj.weight', '_clip.transformer.resblocks.0.mlp.c_proj.bias', '_clip.transformer.resblocks.0.ln_2.weight', '_clip.transformer.resblocks.0.ln_2.bias', '_clip.transformer.resblocks.1.attn.in_proj_weight', '_clip.transformer.resblocks.1.attn.in_proj_bias', '_clip.transformer.resblocks.1.attn.out_proj.weight', '_clip.transformer.resblocks.1.attn.out_proj.bias', '_clip.transformer.resblocks.1.ln_1.weight', '_clip.transformer.resblocks.1.ln_1.bias', '_clip.transformer.resblocks.1.mlp.c_fc.weight', '_clip.transformer.resblocks.1.mlp.c_fc.bias', '_clip.transformer.resblocks.1.mlp.c_proj.weight', '_clip.transformer.resblocks.1.mlp.c_proj.bias', '_clip.transformer.resblocks.1.ln_2.weight', '_clip.transformer.resblocks.1.ln_2.bias', '_clip.transformer.resblocks.2.attn.in_proj_weight', '_clip.transformer.resblocks.2.attn.in_proj_bias', '_clip.transformer.resblocks.2.attn.out_proj.weight', '_clip.transformer.resblocks.2.attn.out_proj.bias', '_clip.transformer.resblocks.2.ln_1.weight', '_clip.transformer.resblocks.2.ln_1.bias', '_clip.transformer.resblocks.2.mlp.c_fc.weight', '_clip.transformer.resblocks.2.mlp.c_fc.bias', '_clip.transformer.resblocks.2.mlp.c_proj.weight', '_clip.transformer.resblocks.2.mlp.c_proj.bias', '_clip.transformer.resblocks.2.ln_2.weight', '_clip.transformer.resblocks.2.ln_2.bias', '_clip.transformer.resblocks.3.attn.in_proj_weight', '_clip.transformer.resblocks.3.attn.in_proj_bias', '_clip.transformer.resblocks.3.attn.out_proj.weight', '_clip.transformer.resblocks.3.attn.out_proj.bias', '_clip.transformer.resblocks.3.ln_1.weight', '_clip.transformer.resblocks.3.ln_1.bias', '_clip.transformer.resblocks.3.mlp.c_fc.weight', '_clip.transformer.resblocks.3.mlp.c_fc.bias', '_clip.transformer.resblocks.3.mlp.c_proj.weight', '_clip.transformer.resblocks.3.mlp.c_proj.bias', '_clip.transformer.resblocks.3.ln_2.weight', '_clip.transformer.resblocks.3.ln_2.bias', '_clip.transformer.resblocks.4.attn.in_proj_weight', '_clip.transformer.resblocks.4.attn.in_proj_bias', '_clip.transformer.resblocks.4.attn.out_proj.weight', '_clip.transformer.resblocks.4.attn.out_proj.bias', '_clip.transformer.resblocks.4.ln_1.weight', '_clip.transformer.resblocks.4.ln_1.bias', '_clip.transformer.resblocks.4.mlp.c_fc.weight', '_clip.transformer.resblocks.4.mlp.c_fc.bias', '_clip.transformer.resblocks.4.mlp.c_proj.weight', '_clip.transformer.resblocks.4.mlp.c_proj.bias', '_clip.transformer.resblocks.4.ln_2.weight', '_clip.transformer.resblocks.4.ln_2.bias', '_clip.transformer.resblocks.5.attn.in_proj_weight', '_clip.transformer.resblocks.5.attn.in_proj_bias', '_clip.transformer.resblocks.5.attn.out_proj.weight', '_clip.transformer.resblocks.5.attn.out_proj.bias', '_clip.transformer.resblocks.5.ln_1.weight', '_clip.transformer.resblocks.5.ln_1.bias', '_clip.transformer.resblocks.5.mlp.c_fc.weight', '_clip.transformer.resblocks.5.mlp.c_fc.bias', '_clip.transformer.resblocks.5.mlp.c_proj.weight', '_clip.transformer.resblocks.5.mlp.c_proj.bias', '_clip.transformer.resblocks.5.ln_2.weight', '_clip.transformer.resblocks.5.ln_2.bias', '_clip.transformer.resblocks.6.attn.in_proj_weight', '_clip.transformer.resblocks.6.attn.in_proj_bias', '_clip.transformer.resblocks.6.attn.out_proj.weight', '_clip.transformer.resblocks.6.attn.out_proj.bias', '_clip.transformer.resblocks.6.ln_1.weight', '_clip.transformer.resblocks.6.ln_1.bias', '_clip.transformer.resblocks.6.mlp.c_fc.weight', '_clip.transformer.resblocks.6.mlp.c_fc.bias', '_clip.transformer.resblocks.6.mlp.c_proj.weight', '_clip.transformer.resblocks.6.mlp.c_proj.bias', '_clip.transformer.resblocks.6.ln_2.weight', '_clip.transformer.resblocks.6.ln_2.bias', '_clip.transformer.resblocks.7.attn.in_proj_weight', '_clip.transformer.resblocks.7.attn.in_proj_bias', '_clip.transformer.resblocks.7.attn.out_proj.weight', '_clip.transformer.resblocks.7.attn.out_proj.bias', '_clip.transformer.resblocks.7.ln_1.weight', '_clip.transformer.resblocks.7.ln_1.bias', '_clip.transformer.resblocks.7.mlp.c_fc.weight', '_clip.transformer.resblocks.7.mlp.c_fc.bias', '_clip.transformer.resblocks.7.mlp.c_proj.weight', '_clip.transformer.resblocks.7.mlp.c_proj.bias', '_clip.transformer.resblocks.7.ln_2.weight', '_clip.transformer.resblocks.7.ln_2.bias', '_clip.transformer.resblocks.8.attn.in_proj_weight', '_clip.transformer.resblocks.8.attn.in_proj_bias', '_clip.transformer.resblocks.8.attn.out_proj.weight', '_clip.transformer.resblocks.8.attn.out_proj.bias', '_clip.transformer.resblocks.8.ln_1.weight', '_clip.transformer.resblocks.8.ln_1.bias', '_clip.transformer.resblocks.8.mlp.c_fc.weight', '_clip.transformer.resblocks.8.mlp.c_fc.bias', '_clip.transformer.resblocks.8.mlp.c_proj.weight', '_clip.transformer.resblocks.8.mlp.c_proj.bias', '_clip.transformer.resblocks.8.ln_2.weight', '_clip.transformer.resblocks.8.ln_2.bias', '_clip.transformer.resblocks.9.attn.in_proj_weight', '_clip.transformer.resblocks.9.attn.in_proj_bias', '_clip.transformer.resblocks.9.attn.out_proj.weight', '_clip.transformer.resblocks.9.attn.out_proj.bias', '_clip.transformer.resblocks.9.ln_1.weight', '_clip.transformer.resblocks.9.ln_1.bias', '_clip.transformer.resblocks.9.mlp.c_fc.weight', '_clip.transformer.resblocks.9.mlp.c_fc.bias', '_clip.transformer.resblocks.9.mlp.c_proj.weight', '_clip.transformer.resblocks.9.mlp.c_proj.bias', '_clip.transformer.resblocks.9.ln_2.weight', '_clip.transformer.resblocks.9.ln_2.bias', '_clip.transformer.resblocks.10.attn.in_proj_weight', '_clip.transformer.resblocks.10.attn.in_proj_bias', '_clip.transformer.resblocks.10.attn.out_proj.weight', '_clip.transformer.resblocks.10.attn.out_proj.bias', '_clip.transformer.resblocks.10.ln_1.weight', '_clip.transformer.resblocks.10.ln_1.bias', '_clip.transformer.resblocks.10.mlp.c_fc.weight', '_clip.transformer.resblocks.10.mlp.c_fc.bias', '_clip.transformer.resblocks.10.mlp.c_proj.weight', '_clip.transformer.resblocks.10.mlp.c_proj.bias', '_clip.transformer.resblocks.10.ln_2.weight', '_clip.transformer.resblocks.10.ln_2.bias', '_clip.transformer.resblocks.11.attn.in_proj_weight', '_clip.transformer.resblocks.11.attn.in_proj_bias', '_clip.transformer.resblocks.11.attn.out_proj.weight', '_clip.transformer.resblocks.11.attn.out_proj.bias', '_clip.transformer.resblocks.11.ln_1.weight', '_clip.transformer.resblocks.11.ln_1.bias', '_clip.transformer.resblocks.11.mlp.c_fc.weight', '_clip.transformer.resblocks.11.mlp.c_fc.bias', '_clip.transformer.resblocks.11.mlp.c_proj.weight', '_clip.transformer.resblocks.11.mlp.c_proj.bias', '_clip.transformer.resblocks.11.ln_2.weight', '_clip.transformer.resblocks.11.ln_2.bias', '_clip.token_embedding.weight', '_clip.ln_final.weight', '_clip.ln_final.bias', 'mmha.mha.in_proj_weight', 'mmha.mha.in_proj_bias', 'mmha.mha.out_proj.weight', 'mmha.mha.out_proj.bias']\n",
      "Parameters to be updated: {'mmha.mha.out_proj.bias', 'mmha.mha.out_proj.weight', 'mmha.mha.in_proj_bias', 'mmha.mha.in_proj_weight'}\n",
      "Number of learnable paramms: 1050624\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/dominykas.seputis/github/fomo/.venv/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:126: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "learner_args = LearnerArgs()\n",
    "learner_args.device = \"cpu\"\n",
    "learner_args.epochs = 100\n",
    "learner_args.model_type = \"clip_transformer\"\n",
    "learner_args.train_eval_size = (100, 200)\n",
    "\n",
    "learner = Learner(learner_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/2]\tTime 13.127 (13.127)\tData  8.908 ( 8.908)\tLoss 2.2597e+00 (2.2597e+00)\tAcc@1  78.12 ( 78.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:26<00:00, 13.19s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:34, 11.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.398 (11.398)\tLoss 2.2532e+00 (2.2532e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "saved best file\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [1][0/2]\tTime 12.346 (12.346)\tData  8.873 ( 8.873)\tLoss 2.2531e+00 (2.2531e+00)\tAcc@1  89.06 ( 89.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.54s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:33, 11.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.228 (11.228)\tLoss 2.2532e+00 (2.2532e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 1 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [2][0/2]\tTime 12.083 (12.083)\tData  8.604 ( 8.604)\tLoss 2.2549e+00 (2.2549e+00)\tAcc@1  85.94 ( 85.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.42s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:34, 11.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.499 (11.499)\tLoss 2.2532e+00 (2.2532e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.58s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 2 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [3][0/2]\tTime 14.047 (14.047)\tData 10.476 (10.476)\tLoss 2.2537e+00 (2.2537e+00)\tAcc@1  87.50 ( 87.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:27<00:00, 13.64s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:35, 11.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.753 (11.753)\tLoss 2.2532e+00 (2.2532e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.66s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [4][0/2]\tTime 12.318 (12.318)\tData  8.715 ( 8.715)\tLoss 2.2554e+00 (2.2554e+00)\tAcc@1  84.38 ( 84.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.66s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:35, 11.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.806 (11.806)\tLoss 2.2531e+00 (2.2531e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:39<00:00,  9.81s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 4 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [5][0/2]\tTime 12.356 (12.356)\tData  8.892 ( 8.892)\tLoss 2.2531e+00 (2.2531e+00)\tAcc@1  87.50 ( 87.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.54s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:35, 11.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.901 (11.901)\tLoss 2.2531e+00 (2.2531e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 5 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/2]\tTime 12.173 (12.173)\tData  8.696 ( 8.696)\tLoss 2.2557e+00 (2.2557e+00)\tAcc@1  82.81 ( 82.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.48s/it]\n",
      " 25%|██▌       | 1/4 [00:12<00:37, 12.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 12.409 (12.409)\tLoss 2.2531e+00 (2.2531e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:40<00:00, 10.02s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 6 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [7][0/2]\tTime 12.159 (12.159)\tData  8.807 ( 8.807)\tLoss 2.2579e+00 (2.2579e+00)\tAcc@1  81.25 ( 81.25)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.47s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:34, 11.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.572 (11.572)\tLoss 2.2531e+00 (2.2531e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.60s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 7 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [8][0/2]\tTime 12.401 (12.401)\tData  8.706 ( 8.706)\tLoss 2.2545e+00 (2.2545e+00)\tAcc@1  84.38 ( 84.38)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:25<00:00, 12.68s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:33, 11.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.285 (11.285)\tLoss 2.2531e+00 (2.2531e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 8 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [9][0/2]\tTime 11.355 (11.355)\tData  8.137 ( 8.137)\tLoss 2.2530e+00 (2.2530e+00)\tAcc@1  89.06 ( 89.06)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:23<00:00, 11.98s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:33, 11.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.089 (11.089)\tLoss 2.2530e+00 (2.2530e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:37<00:00,  9.42s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 9 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [10][0/2]\tTime 11.310 (11.310)\tData  8.120 ( 8.120)\tLoss 2.2550e+00 (2.2550e+00)\tAcc@1  85.94 ( 85.94)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:24<00:00, 12.01s/it]\n",
      " 25%|██▌       | 1/4 [00:11<00:34, 11.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [0/4]\tTime 11.347 (11.347)\tLoss 2.2530e+00 (2.2530e+00)\tPrompt Acc@1  92.19 ( 92.19)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4/4 [00:38<00:00,  9.68s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 95.500\n",
      "There's no improvement for 10 epochs.\n",
      "The training halted by early stopping criterion.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/157 [00:09<25:11,  9.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [  0/157]\tTime  9.689 ( 9.689)\tLoss 2.2538e+00 (2.2538e+00)\tPrompt Acc@1  82.81 ( 82.81)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 11/157 [00:41<07:54,  3.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 10/157]\tTime  3.304 ( 3.753)\tLoss 2.2549e+00 (2.2547e+00)\tPrompt Acc@1  92.19 ( 90.77)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 21/157 [01:12<07:05,  3.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 20/157]\tTime  3.076 ( 3.468)\tLoss 2.2566e+00 (2.2544e+00)\tPrompt Acc@1  92.19 ( 90.92)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|█▉        | 31/157 [01:44<06:35,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 30/157]\tTime  3.057 ( 3.369)\tLoss 2.2573e+00 (2.2542e+00)\tPrompt Acc@1  92.19 ( 91.18)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 41/157 [02:17<06:14,  3.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 40/157]\tTime  3.277 ( 3.357)\tLoss 2.2543e+00 (2.2545e+00)\tPrompt Acc@1  90.62 ( 90.62)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 32%|███▏      | 51/157 [02:51<05:58,  3.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 50/157]\tTime  3.557 ( 3.366)\tLoss 2.2529e+00 (2.2544e+00)\tPrompt Acc@1  89.06 ( 90.59)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 39%|███▉      | 61/157 [03:24<05:07,  3.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 60/157]\tTime  3.133 ( 3.349)\tLoss 2.2530e+00 (2.2544e+00)\tPrompt Acc@1  90.62 ( 90.57)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 71/157 [03:57<04:40,  3.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 70/157]\tTime  3.362 ( 3.339)\tLoss 2.2542e+00 (2.2543e+00)\tPrompt Acc@1  92.19 ( 90.69)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 52%|█████▏    | 81/157 [04:29<04:13,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 80/157]\tTime  3.254 ( 3.333)\tLoss 2.2562e+00 (2.2545e+00)\tPrompt Acc@1  89.06 ( 90.64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 91/157 [05:01<03:29,  3.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [ 90/157]\tTime  3.154 ( 3.317)\tLoss 2.2528e+00 (2.2546e+00)\tPrompt Acc@1  87.50 ( 90.45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 64%|██████▍   | 101/157 [05:33<02:59,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [100/157]\tTime  3.168 ( 3.306)\tLoss 2.2578e+00 (2.2548e+00)\tPrompt Acc@1  92.19 ( 90.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 71%|███████   | 111/157 [06:05<02:25,  3.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [110/157]\tTime  3.173 ( 3.291)\tLoss 2.2531e+00 (2.2549e+00)\tPrompt Acc@1  93.75 ( 90.30)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 121/157 [06:37<01:55,  3.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [120/157]\tTime  3.168 ( 3.287)\tLoss 2.2539e+00 (2.2548e+00)\tPrompt Acc@1  92.19 ( 90.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 131/157 [07:09<01:21,  3.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [130/157]\tTime  3.171 ( 3.276)\tLoss 2.2542e+00 (2.2549e+00)\tPrompt Acc@1  90.62 ( 90.46)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|████████▉ | 141/157 [07:41<00:53,  3.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [140/157]\tTime  3.338 ( 3.275)\tLoss 2.2595e+00 (2.2550e+00)\tPrompt Acc@1  87.50 ( 90.39)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 96%|█████████▌| 151/157 [08:14<00:20,  3.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validate: [150/157]\tTime  3.241 ( 3.275)\tLoss 2.2578e+00 (2.2549e+00)\tPrompt Acc@1  92.19 ( 90.50)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 157/157 [08:51<00:00,  3.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Prompt Acc@1 90.510\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "learner.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 3 and 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      4\u001b[0m img2_tensor \u001b[38;5;241m=\u001b[39m learner\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mtransform(img2)\n\u001b[1;32m      6\u001b[0m img_input \u001b[38;5;241m=\u001b[39m img1_tensor\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_input\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/fomo/fomo/models/clip/clip_transformer.py:75\u001b[0m, in \u001b[0;36mClipTransformer.forward\u001b[0;34m(self, images, prompts)\u001b[0m\n\u001b[1;32m     72\u001b[0m image_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencode_images(images)\n\u001b[1;32m     73\u001b[0m text_features \u001b[38;5;241m=\u001b[39m text_features\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, image_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 75\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimage_features\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m tr_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmini_transformer(inputs)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     78\u001b[0m inputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_outputs\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 3 and 2"
     ]
    }
   ],
   "source": [
    "img1 = Image.open(\"./cat.jpg\")\n",
    "img2 = Image.open(\"./dog.jpg\")\n",
    "img1_tensor = learner.model.transform(img1)\n",
    "img2_tensor = learner.model.transform(img2)\n",
    "\n",
    "img_input = img1_tensor.unsqueeze(0)\n",
    "\n",
    "learner.model.forward(img_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'function' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mlearner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/fomo/fomo/pipelines/train.py:136\u001b[0m, in \u001b[0;36mLearner.run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m epochs_since_improvement \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lr_args\u001b[38;5;241m.\u001b[39mepochs):\n\u001b[0;32m--> 136\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_one_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    137\u001b[0m     acc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lr_args\u001b[38;5;241m.\u001b[39muse_wandb:\n",
      "File \u001b[0;32m~/github/fomo/fomo/pipelines/train.py:199\u001b[0m, in \u001b[0;36mLearner.train_one_epoch\u001b[0;34m(self, epoch)\u001b[0m\n\u001b[1;32m    197\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m    198\u001b[0m targets \u001b[38;5;241m=\u001b[39m targets\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m--> 199\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    201\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion(outputs, targets)\n\u001b[1;32m    203\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/fomo/fomo/models/clip/clip_transformer.py:77\u001b[0m, in \u001b[0;36mClipTransformer.forward\u001b[0;34m(self, images, prompts)\u001b[0m\n\u001b[1;32m     75\u001b[0m inputs \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([text_features, image_features], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;66;03m# 4, 2, 512\u001b[39;00m\n\u001b[0;32m---> 77\u001b[0m tr_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmini_transformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     79\u001b[0m inputs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_outputs\n\u001b[1;32m     81\u001b[0m _text_features, _image_features \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39msplit(\n\u001b[1;32m     82\u001b[0m     [text_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], image_features\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     83\u001b[0m )\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/fomo/fomo/models/clip/clip_transformer.py:36\u001b[0m, in \u001b[0;36mMiniTransformer.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     33\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     34\u001b[0m mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_attn_mask\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mrepeat(batch_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_heads, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 36\u001b[0m output: torch\u001b[38;5;241m.\u001b[39mTensor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/github/fomo/.venv/lib/python3.11/site-packages/torch/nn/modules/activation.py:1116\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[0;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ((attn_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(attn_mask))\n\u001b[1;32m   1113\u001b[0m    \u001b[38;5;129;01mor\u001b[39;00m (key_padding_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mis_floating_point(key_padding_mask)):\n\u001b[1;32m   1114\u001b[0m     why_not_fast_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfloating-point masks are not supported for fast path.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1116\u001b[0m is_batched \u001b[38;5;241m=\u001b[39m \u001b[43mquery\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdim\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[1;32m   1118\u001b[0m key_padding_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1119\u001b[0m     mask\u001b[38;5;241m=\u001b[39mkey_padding_mask,\n\u001b[1;32m   1120\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkey_padding_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1123\u001b[0m     target_type\u001b[38;5;241m=\u001b[39mquery\u001b[38;5;241m.\u001b[39mdtype\n\u001b[1;32m   1124\u001b[0m )\n\u001b[1;32m   1126\u001b[0m attn_mask \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39m_canonical_mask(\n\u001b[1;32m   1127\u001b[0m     mask\u001b[38;5;241m=\u001b[39mattn_mask,\n\u001b[1;32m   1128\u001b[0m     mask_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattn_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1132\u001b[0m     check_other\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   1133\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'function' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "learner.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
